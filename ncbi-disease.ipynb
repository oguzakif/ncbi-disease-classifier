{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATb_O8PhEYL8"
      },
      "source": [
        "# TRAINING A DISEASE MARKER WITH NCBI-DISEASE DATASET \n",
        "This notebook is about marking the words that are disease.\\\n",
        "The next step is defining the branch of disease."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66TQMAPpEYL-"
      },
      "source": [
        "**Import necessary libraries**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZbr992WEYL_",
        "outputId": "b8c88f00-939f-4091-c1d1-75fc400cb043"
      },
      "outputs": [],
      "source": [
        "from datasets import list_datasets, load_dataset\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import nltk as nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dot, Embedding, Flatten\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1c0fviZEYMB"
      },
      "source": [
        "**Load the training and validation datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kzmd3ZjVEYMB",
        "outputId": "bac02eaf-f095-4f86-c938-3c7a501a1bba"
      },
      "outputs": [],
      "source": [
        "ncbi_train = load_dataset(\"ncbi_disease\", split=\"train\")\n",
        "ncbi_validation = load_dataset(\"ncbi_disease\", split=\"validation\")\n",
        "ncbi_test = load_dataset(\"ncbi_disease\", split=\"test\")\n",
        "nltk.download(\"stopwords\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsiZjlOjEYMC"
      },
      "source": [
        "**Convert the datasets into pandas dataframe. Merge the validation and the training dataset then split it with specific ratio**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFWJJ3dXEYMD"
      },
      "outputs": [],
      "source": [
        "train_df = ncbi_train.to_pandas()\n",
        "\n",
        "valid_df = ncbi_validation.to_pandas()\n",
        "# train_df = train_df.append(valid_df)\n",
        "# train_df = train_df.sample(frac=1)\n",
        "# train_df, valid_df = train_test_split(train_df, test_size=0.2)\n",
        "\n",
        "test_df = ncbi_test.to_pandas()\n",
        "pd.set_option('display.max_colwidth', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Define the 2d to 1d converter function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1p1aHQEMEYMF"
      },
      "outputs": [],
      "source": [
        "def conv2d_to_1d(tokens,labels):\n",
        "    token_set = []\n",
        "    label_set = []\n",
        "    for element in tokens:\n",
        "        for item in element:\n",
        "            token_set.append(item)\n",
        "    \n",
        "    for element in labels:\n",
        "        for item in element:\n",
        "            label_set.append(item)\n",
        "\n",
        "    return token_set,label_set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Define the stopword and punctuation remover function from both features and labels**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KW9UHQDaEYMF",
        "outputId": "178f4c0b-4df8-4128-f93c-39c5306675e6"
      },
      "outputs": [],
      "source": [
        "\n",
        "from string import punctuation\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "def remove_stopwords_punctuations(tokens, labels):\n",
        "    new_tokens = []\n",
        "    new_labels = []\n",
        "    flag = True\n",
        "    for index in range(len(tokens)):\n",
        "        formatted_token = str(tokens[index]).strip().lower()\n",
        "        stop_count = stop.count(formatted_token)\n",
        "        punctuation_count = punctuation.count(formatted_token)\n",
        "\n",
        "        if(stop_count == 0) and (punctuation_count == 0):\n",
        "            new_tokens.append(str(tokens[index]).strip().lower())\n",
        "            new_labels.append(labels[index])\n",
        "\n",
        "    return new_tokens,new_labels\n",
        "   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Define the vocab size and embedding dimensions for the data shape**\\\n",
        "*Convert the data into 2d to 1d array in order to be sequenced\\\n",
        "*Remove all stopword and punctuations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPK1uKBgEYMG"
      },
      "outputs": [],
      "source": [
        "vocab_size = 12000\n",
        "embedding_dim = 16\n",
        "max_length = 50\n",
        "trunc_type='post'\n",
        "pad_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "train_set,train_label_set = conv2d_to_1d(train_df.tokens.values, train_df.ner_tags)\n",
        "valid_set, valid_label_set = conv2d_to_1d(valid_df.tokens.values, valid_df.ner_tags)\n",
        "test_set, test_label_set = conv2d_to_1d(test_df.tokens.values, test_df.ner_tags)\n",
        "\n",
        "train_set, train_label_set = remove_stopwords_punctuations(train_set,train_label_set)\n",
        "valid_set, valid_label_set = remove_stopwords_punctuations(valid_set,valid_label_set)\n",
        "test_set, test_label_set = remove_stopwords_punctuations(test_set,test_label_set)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Pad the sequences so that they are all the same length**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRxzVM57EYMG",
        "outputId": "b080d382-9334-4086-b534-c35a76f8bfae"
      },
      "outputs": [],
      "source": [
        "tokenizer.fit_on_texts(train_set)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "training_sequences = tokenizer.texts_to_sequences(train_set)\n",
        "training_padded = pad_sequences(training_sequences,maxlen=max_length, \n",
        "                                truncating=trunc_type, padding=pad_type)\n",
        "\n",
        "validation_sequences = tokenizer.texts_to_sequences(valid_set)\n",
        "validation_padded = pad_sequences(validation_sequences,maxlen=max_length, \n",
        "                                truncating=trunc_type, padding=pad_type)\n",
        "\n",
        "training_labels_final = np.array(train_label_set)\n",
        "validation_labels_final = np.array(valid_label_set)\n",
        "\n",
        "\n",
        "print(training_padded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmC5m9iamg3u"
      },
      "source": [
        "**Evaluate the test accuracy of the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgNRVtDZEYMG"
      },
      "outputs": [],
      "source": [
        "def predict_tokens(model_type,model, tokens,token_labels):\n",
        "  # Create the sequences\n",
        "  padding_type='post'\n",
        "  sample_sequences = tokenizer.texts_to_sequences(tokens)\n",
        "  tokens_padded = pad_sequences(sample_sequences, padding=padding_type, \n",
        "                                 maxlen=max_length) \n",
        "  \n",
        "  results = model.evaluate(tokens_padded,token_labels)\n",
        "  print(\"test loss, test acc:\", results, \" <-> \",model_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Plot the given type history from model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdpUeJ6mEYMH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Implement the training function for each type of model**\\\n",
        "*Define an early stopping callback with patience 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "antM4VgfEYMH"
      },
      "outputs": [],
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
        "\n",
        "def fit_model_and_show_results (model, num_epochs,train_padded, train_label, valid_padded, valid_label):\n",
        "  model.summary()\n",
        "  history = model.fit(train_padded, train_label, epochs=num_epochs, \n",
        "                      validation_data=(valid_padded, valid_label), callbacks=[callback])\n",
        "  plot_graphs(history, \"accuracy\")\n",
        "  plot_graphs(history, \"loss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ly6efdu2EYMH"
      },
      "source": [
        "**Use only Embedding to train the network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-2T0QZb6EYMH",
        "outputId": "d9855f9a-c840-42b7-e33f-7c2fa3f18027"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),  \n",
        "    tf.keras.layers.Dropout(0.6),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), \n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "fit_model_and_show_results(model, num_epochs=100, train_padded=training_padded, train_label=training_labels_final, valid_padded=validation_padded, valid_label=validation_labels_final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swPHjbdvEYMH"
      },
      "source": [
        "**Use a CNN model to train network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nIvxhT--EYMI",
        "outputId": "2712de55-0209-4d44-be23-093972b0c2a4"
      },
      "outputs": [],
      "source": [
        "model_cnn = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Conv1D(16, 2, activation='relu'),\n",
        "    tf.keras.layers.GlobalMaxPooling1D(),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model_cnn.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.00007), \n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "fit_model_and_show_results(model_cnn, num_epochs=50, train_padded=training_padded, train_label=training_labels_final, valid_padded=validation_padded, valid_label=validation_labels_final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVC2KglbEYMK"
      },
      "source": [
        "**Use a GRU model to train the network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 779
        },
        "id": "H_ad3oYrEYMK",
        "outputId": "ae04d93d-2fe3-46ac-e29f-1e9429add40e"
      },
      "outputs": [],
      "source": [
        "model_gru = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "model_gru.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.00007), \n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "fit_model_and_show_results(model_gru,  num_epochs=20, train_padded=training_padded, train_label=training_labels_final, valid_padded=validation_padded, valid_label=validation_labels_final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVftJEjjEYML"
      },
      "source": [
        "**Use a Bidirectional LSTM model to train the network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hN1ICKI0EYML",
        "outputId": "659c6a62-46dc-43f2-c6aa-3f328457a99e"
      },
      "outputs": [],
      "source": [
        "model_bidi_lstm = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)), \n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "model_bidi_lstm.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.00007), \n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "fit_model_and_show_results(model_bidi_lstm,  num_epochs=20, train_padded=training_padded, train_label=training_labels_final, valid_padded=validation_padded, valid_label=validation_labels_final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K40WLuOEYMM"
      },
      "source": [
        "**Use a Multiple Bidirectional LSTMs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOTJR3_aEYMM",
        "outputId": "63c70129-5384-463c-9eb5-6c7a0f0c57f1"
      },
      "outputs": [],
      "source": [
        "model_multiple_bidi_lstm = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim, \n",
        "                                                       return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "model_multiple_bidi_lstm.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.00007), \n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "fit_model_and_show_results(model_multiple_bidi_lstm, num_epochs=20, train_padded=training_padded, train_label=training_labels_final, valid_padded=validation_padded, valid_label=validation_labels_final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e58UGFCEYMM"
      },
      "source": [
        "**Compare the models with the Test Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAKORXN8EYMN",
        "outputId": "37e31f47-558c-49a2-ee87-50202bba9957"
      },
      "outputs": [],
      "source": [
        "predict_tokens(\"Only Embedding Model\",model, test_set, np.array(test_label_set))\n",
        "predict_tokens(\"CNN model\",model_cnn, test_set, np.array(test_label_set))\n",
        "predict_tokens(\"GRU model\",model_gru, test_set, np.array(test_label_set))\n",
        "predict_tokens(\"Bidi LSTM model\",model_bidi_lstm, test_set, np.array(test_label_set))\n",
        "predict_tokens(\"Multiple Bidi LSTM model\",model_multiple_bidi_lstm, test_set, np.array(test_label_set))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ncbi-disease.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
